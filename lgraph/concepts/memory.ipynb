{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Memory](https://langchain-ai.github.io/langgraph/concepts/memory/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有两种 Short-term memory 和 Long-term memory。\n",
    "\n",
    "Short-term memory： 就是 thread 级别的 memory，通过 checkpointer 存储在 state 中。\n",
    "\n",
    "Long-term memory：跨 thread 的 memory，通过 stores 存储。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short-term memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Managing long conversation history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "管理短期记忆是一种在精确性和召回率与应用程序的其他性能要求（延迟和成本）之间进行平衡的实践。\n",
    "\n",
    "提到了两种技术：\n",
    "\n",
    "**Editing message lists**: How to think about trimming and filtering a list of messages before passing to language model.\n",
    "\n",
    "**Summarizing past conversations**: A common technique to use when you don't just want to filter the list of messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Editing message lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most direct approach is to remove old messages from a list (similar to a least-recently used cache)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Filter Mesages](https://langchain-ai.github.io/langgraph/concepts/img/memory/filter.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 看上去 reducer 的第一个参数是 state 对应 key 的values，第二个参数是 node 返回的 updates\n",
    "def manage_list(existing: list, updates: Union[list, dict]):\n",
    "    if isinstance(updates, list):\n",
    "        # Normal case, add to the history\n",
    "        return existing + updates\n",
    "    elif isinstance(updates, dict) and updates[\"type\"] == \"keep\":\n",
    "        # You get to decide what this looks like.\n",
    "        # For example, you could simplify and just accept a string \"DELETE\"\n",
    "        # and clear the entire list.\n",
    "        return existing[updates[\"from\"]:updates[\"to\"]]\n",
    "    # etc. We define how to interpret updates\n",
    "\n",
    "class State(TypedDict):\n",
    "    my_list: Annotated[list, manage_list]\n",
    "\n",
    "def my_node(state: State):\n",
    "    return {\n",
    "        # We return an update for the field \"my_list\" saying to\n",
    "        # keep only values from index -5 to the end (deleting the rest)\n",
    "        \"my_list\": {\"type\": \"keep\", \"from\": -5, \"to\": None}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import RemoveMessage, AIMessage\n",
    "from langgraph.graph import add_messages\n",
    "# ... other imports\n",
    "\n",
    "class State(TypedDict):\n",
    "    # add_messages will default to upserting messages by ID to the existing list\n",
    "    # if a RemoveMessage is returned, it will delete the message in the list by ID\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "def my_node_1(state: State):\n",
    "    # Add an AI message to the `messages` list in the state\n",
    "    return {\"messages\": [AIMessage(content=\"Hi\")]}\n",
    "\n",
    "def my_node_2(state: State):\n",
    "    # Delete all but the last 2 messages from the `messages` list in the state\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state['messages'][:-2]]\n",
    "    return {\"messages\": delete_messages}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summarizing past conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![summarize](https://langchain-ai.github.io/langgraph/concepts/img/memory/summary.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "class State(MessagesState):\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_conversation(state: State):\n",
    "\n",
    "    # First, we get any existing summary\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # Create our summarization prompt\n",
    "    if summary:\n",
    "\n",
    "        # A summary already exists\n",
    "        summary_message = (\n",
    "            f\"This is a summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above:\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        summary_message = \"Create a summary of the conversation above:\"\n",
    "\n",
    "    # Add prompt to our history\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    # Delete all but the 2 most recent messages\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Knowing when to remove messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import trim_messages\n",
    "trim_messages(\n",
    "    messages,\n",
    "    # Keep the last <= n_count tokens of the messages.\n",
    "    strategy=\"last\",\n",
    "    # Remember to adjust based on your model\n",
    "    # or else pass a custom token_encoder\n",
    "    token_counter=ChatOpenAI(model=\"gpt-4\"),\n",
    "    # Remember to adjust based on the desired conversation\n",
    "    # length\n",
    "    max_tokens=45,\n",
    "    # Most chat models expect that chat history starts with either:\n",
    "    # (1) a HumanMessage or\n",
    "    # (2) a SystemMessage followed by a HumanMessage\n",
    "    start_on=\"human\",\n",
    "    # Most chat models expect that chat history ends with either:\n",
    "    # (1) a HumanMessage or\n",
    "    # (2) a ToolMessage\n",
    "    end_on=(\"human\", \"tool\"),\n",
    "    # Usually, we want to keep the SystemMessage\n",
    "    # if it's present in the original history.\n",
    "    # The SystemMessage has special instructions for the model.\n",
    "    include_system=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long-term memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Storing memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "\n",
    "def embed(texts: list[str]) -> list[list[float]]:\n",
    "    # Replace with an actual embedding function or LangChain embeddings object\n",
    "    return [[1.0, 2.0] * len(texts)]\n",
    "\n",
    "\n",
    "# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use.\n",
    "store = InMemoryStore(index={\"embed\": embed, \"dims\": 2})\n",
    "user_id = \"my-user\"\n",
    "application_context = \"chitchat\"\n",
    "namespace = (user_id, application_context)\n",
    "store.put(\n",
    "    namespace,\n",
    "    \"a-memory\",\n",
    "    {\n",
    "        \"rules\": [\n",
    "            \"User likes short, direct language\",\n",
    "            \"User only speaks English & python\",\n",
    "        ],\n",
    "        \"my-key\": \"my-value\",\n",
    "    },\n",
    ")\n",
    "# get the \"memory\" by ID\n",
    "item = store.get(namespace, \"a-memory\")\n",
    "# search for \"memories\" within this namespace, filtering on content equivalence, sorted by vector similarity\n",
    "items = store.search(\n",
    "    namespace, filter={\"my-key\": \"my-value\"}, query=\"language preferences\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Framework for thinking about long-term memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提到了两个问题，但是没看明白想说啥：\n",
    "\n",
    "What is the type of memory?\n",
    "\n",
    "When do you want to update memories?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在记忆上做了一个类比：\n",
    "\n",
    "| Memory Type | What is Stored | Human Example              | Agent Example       |\n",
    "| :---------- | :------------- | :------------------------- | :------------------ |\n",
    "| Semantic    | Facts          | Things I learned in school | Facts about a user  |\n",
    "| Episodic    | Experiences    | Things I did               | Past agent actions  |\n",
    "| Procedural  | Instructions   | Instincts or motor skills  | Agent system prompt |\n",
    "\n",
    "关键是怎么理解这三种记忆呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Semantic Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提到了两种方式：Profile 和 Collection。简单理解，前者是单一的文档，后者是多个文档。\n",
    "\n",
    "A profile is generally just a JSON document with various key-value pairs you've selected to represent your domain.\n",
    "\n",
    "Alternatively, memories can be a collection of documents that are continuously updated and extended over time. \n",
    "\n",
    "重要的一点：这两者都需要维护更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Episodic Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Episodic memory, in both humans and AI agents, involves recalling past events or actions.\n",
    "\n",
    "In practice, episodic memories are often implemented through few-shot example prompting, where agents learn from past sequences to perform tasks correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Procedural Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提到了两种方式：In the hot path、In the background。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
